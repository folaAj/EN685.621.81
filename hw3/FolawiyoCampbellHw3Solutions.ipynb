{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Problem 1</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def expectation_maximization(clusters, num_iterations, data, initial_mu, initial_sigma, initial_p, should_print):\n",
    "    \"\"\" Performs expectation maximiazation the data and prints out the mean, standard deviation, mixing \n",
    "    probabilities and posterior probabilities at each step.\n",
    "    \n",
    "    Params:\n",
    "    clusters: number of clusters\n",
    "    num_iterations: number of iteraions to run EM for\n",
    "    data: data on which EM is run\n",
    "    initial_mu: initial mean\n",
    "    initial_sigma: initial standard deviation\n",
    "    initial_p: initital mixing probability\n",
    "    should_print: indicates if we should print the paremeters at each step\n",
    "    \n",
    "    Returns:\n",
    "    Calculated mean, standard deviation and mixinf probability \n",
    "    \"\"\"\n",
    "    k = clusters\n",
    "    mu = initial_mu\n",
    "    sigma = initial_sigma\n",
    "    p = initial_p\n",
    "    \n",
    "    N, D = data.shape\n",
    "    for iteration in range(0, num_iterations):\n",
    "        prob = []\n",
    "        for i in range (0, N):\n",
    "            prob.append(expectation(p, mu, sigma, data[i], k, D))\n",
    "        prob = np.array(prob)\n",
    "        mu, sigma, p = maximization(prob, data, k)\n",
    "        if(should_print):\n",
    "            print(\"***** iteration {} ******\".format(iteration))\n",
    "            print(\"mean: {}\".format( mu))\n",
    "            print(\"standar deviation: {}\".format( sigma))\n",
    "            print(\"mixing probability: {}\".format(p))\n",
    "            print(\"posterior probability{}\".format(np.array(prob)))\n",
    "            print(\"\\n\")\n",
    "    \n",
    "    return mu, sigma, p\n",
    "\n",
    "def expectation(p, mu, sigma, row, k, D):    \n",
    "    sum = 0\n",
    "    for i in range(0, k):\n",
    "        sum+=p[i]*g(mu[i], sigma[i], row, D)\n",
    "        \n",
    "    prob = []\n",
    "    for i in range(0, k):\n",
    "        prob.append((p[i]*g(mu[i], sigma[i], row, D))/sum)\n",
    "    return np.array(prob)\n",
    "\n",
    "def maximization(prob, data, k):\n",
    "    N, D = data.shape\n",
    "    # Maximixe mean\n",
    "    mean_top = []\n",
    "    for cluster in range(0, k):\n",
    "        sum = 0\n",
    "        for i in range(0, N):\n",
    "            sum+=(prob.transpose()[cluster][i] * data[i])\n",
    "        mean_top.append(sum)\n",
    "    mean_top = np.array(mean_top)\n",
    "    \n",
    "    mean_bottom = []\n",
    "    for cluster in range(0, k):\n",
    "        sum = 0\n",
    "        for i in range(0, N):\n",
    "            sum+=prob.transpose()[cluster][i]\n",
    "        mean_bottom.append(sum)\n",
    "    mean_bottom = np.array(mean_bottom)\n",
    "    mu = mean_top/mean_bottom.reshape(-1,1)\n",
    "    \n",
    "    #Maximize sigma\n",
    "    sigma_top = []\n",
    "    for cluster in range(0, k):\n",
    "        sum = 0\n",
    "        for i in range(0, N):\n",
    "            inter = data[i] - mu[cluster]\n",
    "            sum+=(prob.transpose()[cluster][i] * inter.dot(inter))\n",
    "        sigma_top.append(sum)\n",
    "    sigma_top = np.array(sigma_top)\n",
    "    sigma = np.sqrt(sigma_top/(2 * mean_bottom))\n",
    "    \n",
    "    # Maximize mixing probabilites \n",
    "    p = mean_bottom/N\n",
    "    \n",
    "    return mu, sigma, p\n",
    "    \n",
    "        \n",
    "\n",
    "def g(mu, sigma, row, D):\n",
    "    first = 1/np.power(np.sqrt(2 * np.pi )* sigma, D)\n",
    "    inter = (row-mu)/sigma\n",
    "    second = np.exp(-0.5* inter.dot(inter))\n",
    "    calc =  first * second \n",
    "    return calc\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.5, 0.5])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[1, 2],[4, 2],[1, 3],[4, 3]]\n",
    "np.array(data).std(axis =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** iteration 0 ******\n",
      "mean: [[1.62322012 2.47791236]\n",
      " [3.69837745 2.53018925]]\n",
      "standar deviation: [0.9302605  0.72903364]\n",
      "mixing probability: [0.57748752 0.42251248]\n",
      "posterior probability[[0.93024668 0.06975332]\n",
      " [0.27574969 0.72425031]\n",
      " [0.89983426 0.10016574]\n",
      " [0.20411943 0.79588057]]\n",
      "\n",
      "\n",
      "***** iteration 1 ******\n",
      "mean: [[1.10697853 2.49926496]\n",
      " [3.99551753 2.50078913]]\n",
      "standar deviation: [0.52890956 0.36292334]\n",
      "mixing probability: [0.51774185 0.48225815]\n",
      "posterior probability[[0.99861806 0.00138194]\n",
      " [0.03838789 0.96161211]\n",
      " [0.99849966 0.00150034]\n",
      " [0.03546179 0.96453821]]\n",
      "\n",
      "\n",
      "***** iteration 2 ******\n",
      "mean: [[1.0000008 2.5      ]\n",
      " [4.        2.5      ]]\n",
      "standar deviation: [0.35355508 0.35355339]\n",
      "mixing probability: [0.50000013 0.49999987]\n",
      "posterior probability[[1.00000000e+00 1.95794875e-15]\n",
      " [2.67286455e-07 9.99999733e-01]\n",
      " [1.00000000e+00 1.97489685e-15]\n",
      " [2.64992665e-07 9.99999735e-01]]\n",
      "\n",
      "\n",
      "***** iteration 3 ******\n",
      "mean: [[1.  2.5]\n",
      " [4.  2.5]]\n",
      "standar deviation: [0.35355339 0.35355339]\n",
      "mixing probability: [0.5 0.5]\n",
      "posterior probability[[1.00000000e+00 2.31952159e-16]\n",
      " [2.32036871e-16 1.00000000e+00]\n",
      " [1.00000000e+00 2.31952161e-16]\n",
      " [2.32036869e-16 1.00000000e+00]]\n",
      "\n",
      "\n",
      "***** iteration 4 ******\n",
      "mean: [[1.  2.5]\n",
      " [4.  2.5]]\n",
      "standar deviation: [0.35355339 0.35355339]\n",
      "mixing probability: [0.5 0.5]\n",
      "posterior probability[[1.00000000e+00 2.31952283e-16]\n",
      " [2.31952283e-16 1.00000000e+00]\n",
      " [1.00000000e+00 2.31952283e-16]\n",
      " [2.31952283e-16 1.00000000e+00]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the initial variables provided in the lecture slides.\n",
    "mu = np.array([[2.1766, 2.3922],[3.7571, 2.9190]])\n",
    "sigma = np.array([1.1547, 1.1547])\n",
    "p = np.array([0.5, 0.5])\n",
    "mu, sigma, p = expectation_maximization(2, 5, np.array(data), mu, sigma, p, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Problem 2 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "iris = sns.load_dataset(\"iris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data_cleaned = iris.drop(columns=['species']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_mu = iris_data_cleaned.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_sigma  = iris_data_cleaned.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = 3\n",
    "mu = (calculated_mu.transpose().reshape(-1,1) * np.ones(clusters)) + (calculated_sigma.reshape(-1,1) * np.random.randn(clusters))\n",
    "mu = mu.transpose()\n",
    "sigma = (calculated_sigma.sum()/len(calculated_sigma)) *np.ones(clusters)\n",
    "p = np.ones(clusters)/clusters\n",
    "mu, sigma, p = expectation_maximization(clusters, 100, iris_data_cleaned, mu, sigma, p, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.92337106, 2.75769455, 4.43209925, 1.45007568],\n",
       "       [5.00600096, 3.42798938, 1.46201753, 0.24600965],\n",
       "       [6.82873748, 3.06330432, 5.69913517, 2.05410915]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.59719306, 0.38926979, 0.60782215])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.41731006, 0.33333706, 0.24935288])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sepal_length    5.006\n",
       "sepal_width     3.428\n",
       "petal_length    1.462\n",
       "petal_width     0.246\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris[iris['species']=='setosa'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sepal_length    6.588\n",
       "sepal_width     2.974\n",
       "petal_length    5.552\n",
       "petal_width     2.026\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris[iris['species']=='virginica'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sepal_length    5.936\n",
       "sepal_width     2.770\n",
       "petal_length    4.260\n",
       "petal_width     1.326\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris[iris['species']=='versicolor'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the means it looks like the tree clusters look similar to the actual clusters of setosa, virginica and versicolor as the actual means are close to the final calculated means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
